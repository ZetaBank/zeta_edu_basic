<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Mission &mdash; ZetaBank - Instructor Version</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/toc_custom.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/sphinxcontrib-images\LightBox2\lightbox2\dist\css\lightbox.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/toc_custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/sphinx_highlight.js"></script>
        <script src="../../../_static/sphinxcontrib-images\LightBox2\lightbox2\dist\js\lightbox-plus-jquery.min.js"></script>
        <script src="../../../_static/sphinxcontrib-images\LightBox2\lightbox2-customize\jquery-noconflict.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Discussion" href="6.discussion.html" />
    <link rel="prev" title="Coding Explanation" href="4.coding_explanation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            Elementary Version
              <img src="../../../_static/logo.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                0.1.3
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Day1</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Day1/0.index.html">Enjoy AI robot with body</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../Day1/1.intro/1.intro.html">Let’s go to AI Exploration Education</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../Day1/1.intro/2.team_and_naming.html">Team assignment &amp; Team name</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../Day1/1.intro/3.catch_monster.html">Catch a troublemaker monster</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../Day1/1.intro/4.boss_introduce.html">This is the Boss Monster!</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../Day1/2.robot_arm/0.index.html">Control AI Robot Arm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../Day1/2.robot_arm/1.follow_along.html">Follow Along!</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../Day1/2.robot_arm/2.mission.html">Mission</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../Day1/3.arm_dance/0.index.html">Dance with AI Robot Arm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../Day1/3.arm_dance/1.follow_along.html">Follow Along!</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../Day1/3.arm_dance/2.mission.html">Mission</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../Day1/4.dancing_arm.html">Let’s Introduce Our Team with Music</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Day1/5.gotcha.html">Let’s Pick a Toy with a AI Robot</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Day2</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../0.index.html">Enjoy AI robot with eyes</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../0.index.html">AI training with the body and eyes</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../5.before_ai_education.html">Before you get AI training…</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="1.index.html">Object Detection with SSD-Mobilenet-v2</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="2.follow_along.html">Follow Along!</a></li>
<li class="toctree-l4"><a class="reference internal" href="3.explanation.html">Overall Explanation</a></li>
<li class="toctree-l4"><a class="reference internal" href="4.coding_explanation.html">Coding Explanation</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">Mission</a></li>
<li class="toctree-l4"><a class="reference internal" href="6.discussion.html">Discussion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../2.tao_peoplenet/1.index.html">Tao Peoplenet Examples</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../2.tao_peoplenet/2.follow_along.html">Follow Along!</a></li>
<li class="toctree-l4"><a class="reference internal" href="../2.tao_peoplenet/3.explanation.html">Overall Explanation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../2.tao_peoplenet/4.coding_explanation.html">Coding Explanation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../2.tao_peoplenet/5.mission.html">Mission</a></li>
<li class="toctree-l4"><a class="reference internal" href="../2.tao_peoplenet/6.discussion.html">Discussion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../3.segmentation/1.index.html">Sematic Segmentation Examples</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../3.segmentation/2.follow_along.html">Follow Along!</a></li>
<li class="toctree-l4"><a class="reference internal" href="../3.segmentation/3.explanation.html">Overall Explanation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../3.segmentation/4.coding_explanation.html">Coding Explanation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../3.segmentation/5.mission.html">Mission</a></li>
<li class="toctree-l4"><a class="reference internal" href="../3.segmentation/6.discussion.html">Discussion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../4.posenet/1.index.html">Body Pose Estimation with Pose-ResNet18-Body</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../4.posenet/2.follow_along.html">Follow Along!</a></li>
<li class="toctree-l4"><a class="reference internal" href="../4.posenet/3.explanation.html">Overall Explanation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../4.posenet/4.coding_explanation.html">Coding Explanation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../4.posenet/5.mission.html">Mission</a></li>
<li class="toctree-l4"><a class="reference internal" href="../4.posenet/6.discussion.html">Discussion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../6.after_ai_education.html">After you’ve finished AI training…</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../2.robot_drive/0.index.html">Control AI Robot</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../2.robot_drive/1.follow_along.html">Follow Along!</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../3.monitor_sensor/0.index.html">Monitor AI Robot Sensor</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../3.monitor_sensor/1.follow_along.html">Follow Along!</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../4.drive_team.html">Control AI Robot as a Team</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../5.clean_earth/1.index.html">We save the earth</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../5.clean_earth/2.recycling_garbage.html">Introduction to Recycling Garbage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../5.clean_earth/3.clean_earth.html">Let’s Make a Clean Earth Using AI Robot</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Day3</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Day3/0.index.html">Discuss AI robots</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../Day3/1.background/1.index.html">Background Removal and Background Change</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../Day3/1.background/2.follow_along.html">Follow Along!</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../Day3/1.background/3.explanation.html">Overall Explanation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../Day3/1.background/4.coding_explanation.html">BackgroundNet Coding Explanation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../Day3/1.background/5.mission.html">Mission</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../Day3/1.background/6.discussion.html">Discussion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../Day3/2.presentation.html">Presentation and discussion</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Elementary Version</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../0.index.html">Enjoy AI robot with eyes</a></li>
          <li class="breadcrumb-item"><a href="../0.index.html">AI training with the body and eyes</a></li>
          <li class="breadcrumb-item"><a href="1.index.html">Object Detection with SSD-Mobilenet-v2</a></li>
      <li class="breadcrumb-item active">Mission</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/Day2/2.AI/1.object_detect/5.mission.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="mission">
<h1>Mission<a class="headerlink" href="#mission" title="Permalink to this heading"></a></h1>
<div style="background: #ffe5b4" class="admonition note custom">
    <p style="background: #ffbf00" class="admonition-title">
        Project Name: Custom Pose Estimation System
    </p>
    <div class="line-block">
        <div class="line"><strong>-</strong> This mission is an <strong>individual project</strong></div>
        <div class="line"><strong>-</strong> First, create the custom poseNet program which utilizes zetabot camera.</div>
        <div class="line"><strong>-</strong> Second, adjust the overlay settings of our result. </div>
        <div class="line"><strong>-</strong> Within your individual computers, execute the following mission.  </div>
    </div>
</div><section id="writing-custom-detectnet-program">
<h2>Writing Custom detectNet Program<a class="headerlink" href="#writing-custom-detectnet-program" title="Permalink to this heading"></a></h2>
<p class="linemarker linemarker-28">Similar to how we created a new python file in our team assignment, generate a new python file and name it <code class="docutils literal notranslate"><span class="pre">02_6-2.</span> <span class="pre">object_detection_camera.py</span></code>.</p>
<p class="linemarker linemarker-30">Create a new python file in the Jupyter Notebook Environment:</p>
<ul>
<li><p class="linemarker linemarker-32">Press the blue plus button on the top left corner of the web.</p>
<a class=""
               data-lightbox="group-66d45ade-09c9-46dc-9504-0a5f8e5ab231"
               href="../../../_images/add_plus1.png"
               title=""
               data-title=""
               
               ><img src="../../../_images/add_plus1.png"
                     class=""
                     width="100%"
                     height="auto"
                     alt=""/>
                </a></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<ul>
<li><p class="linemarker linemarker-38">Create a new python file by pressing the <code class="docutils literal notranslate"><span class="pre">Python</span> <span class="pre">File</span></code> button</p>
<a class=""
               data-lightbox="group-6cd3e05b-83c0-484f-b6db-0e8472be3166"
               href="../../../_images/pick_python1.png"
               title=""
               data-title=""
               
               ><img src="../../../_images/pick_python1.png"
                     class=""
                     width="100%"
                     height="auto"
                     alt=""/>
                </a></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<ul>
<li><p class="linemarker linemarker-44">Rename the untitiled python file to <code class="docutils literal notranslate"><span class="pre">02_6-2.</span> <span class="pre">object_detection_camera.py</span></code></p></li>
<li><p class="linemarker linemarker-47">On the new python file, import the libraries necessary. For our Pose Estimation task, we need to import the Jetson inference library modules and jetson utility library modules</p>
<ul class="simple">
<li><p class="linemarker linemarker-49"><code class="docutils literal notranslate"><span class="pre">argparse</span></code>: This library contains modules that are responsbile for bringing and intitializing the flags or parameters set by the user when envoking the program.</p></li>
<li><p class="linemarker linemarker-50"><code class="docutils literal notranslate"><span class="pre">sys</span></code>: this library allows us to manipulate/ utilize system functions within our python programs.</p></li>
<li><p class="linemarker linemarker-52"><code class="docutils literal notranslate"><span class="pre">jetson_inference</span></code>: This library contains all the pre-built networks that can be used for inference task and a functions that would allow for custom models to be used for inference tasks.</p>
<ul>
<li><p class="linemarker linemarker-54"><code class="docutils literal notranslate"><span class="pre">detectNet</span></code>: We are importing detectNet module for our detect task.</p></li>
</ul>
</li>
<li><p class="linemarker linemarker-57"><code class="docutils literal notranslate"><span class="pre">jetson_utils</span></code>: This library contains modules that are responsible for processing input and output sources along with output stream methods. We will be importing the following modules:</p>
<ul>
<li><p class="linemarker linemarker-59"><code class="docutils literal notranslate"><span class="pre">videoSource</span></code>: used to process input source (whether it is a camera, an image, or a video).</p></li>
<li><p class="linemarker linemarker-60"><code class="docutils literal notranslate"><span class="pre">videoOutput</span></code>: used to process the output stream.</p></li>
<li><p class="linemarker linemarker-61"><code class="docutils literal notranslate"><span class="pre">cudaFont</span></code>: this module allows for overlay on the output stream.</p></li>
</ul>
</li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="kn">from</span> <span class="nn">jetson_inference</span> <span class="kn">import</span> <span class="n">poseNet</span>
<span class="kn">from</span> <span class="nn">jetson_utils</span> <span class="kn">import</span> <span class="n">videoSource</span><span class="p">,</span> <span class="n">videoOutput</span><span class="p">,</span> <span class="n">cudaFont</span>
</pre></div>
</div>
</li>
<li><p class="linemarker linemarker-72">After all the libraries are imported, initialize the parser variable with <code class="docutils literal notranslate"><span class="pre">argparse.ArgumentParser</span></code> module.</p>
<p class="linemarker linemarker-74">For our mission, we must receive the network name, and Camera output channel name. Additionally we add 2 other optional parsers.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># parse the command line</span>
<span class="c1"># For our mission, We recieve the network name, and Camera name.</span>
<span class="c1"># Set up argument parser, so that command line parameters can be read within the program</span>
<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Locate objects in a live camera stream using an object detection DNN.&quot;</span><span class="p">,</span>
                                <span class="n">formatter_class</span><span class="o">=</span><span class="n">argparse</span><span class="o">.</span><span class="n">RawTextHelpFormatter</span><span class="p">,</span>
                                <span class="n">epilog</span><span class="o">=</span><span class="n">detectNet</span><span class="o">.</span><span class="n">Usage</span><span class="p">()</span> <span class="o">+</span> <span class="n">videoSource</span><span class="o">.</span><span class="n">Usage</span><span class="p">()</span> <span class="o">+</span> <span class="n">videoOutput</span><span class="o">.</span><span class="n">Usage</span><span class="p">())</span>
<span class="c1"># Major Functionality parameters (required from the user)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;input_CAMERA&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">nargs</span><span class="o">=</span><span class="s1">&#39;?&#39;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;use csi://0 for Raspberry pi Camera&quot;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--network&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;pre-trained model to load&quot;</span><span class="p">)</span>

<span class="c1"># Minor Functionality parameters (optional)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--overlay&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;box,labels,conf&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;detection overlay flags (e.g. --overlay=box,labels,conf)</span><span class="se">\n</span><span class="s2">valid combinations are:  &#39;box&#39;, &#39;labels&#39;, &#39;conf&#39;, &#39;none&#39;&quot;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--threshold&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;minimum detection threshold to use&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p class="linemarker linemarker-92">Initialize opt variable to hold all the user-set flags in a list form. If the user has set no flags, terminate the program:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># If no parameter is given from the user, shut the program down</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_known_args</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">except</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">print_help</span><span class="p">()</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p class="linemarker linemarker-104">Initialize the necessary variables. Since we wish to infer a network with a camera and show the results with our output stream we will need:</p>
<ol class="arabic simple">
<li><p class="linemarker linemarker-106"><code class="docutils literal notranslate"><span class="pre">net</span></code> variable for holding the nvidia pre-built networks. For this mission we are using detectNet network.</p></li>
<li><p class="linemarker linemarker-107"><code class="docutils literal notranslate"><span class="pre">input</span></code> variable for handling the input stream. Using the <code class="docutils literal notranslate"><span class="pre">opt</span></code> variable created in our previous step, we will bring in input_CAMERA to set our videoSource.</p></li>
<li><p class="linemarker linemarker-108"><code class="docutils literal notranslate"><span class="pre">display</span></code> variable for handling the output stream. Although we are accessing the code remotely on our remote computer, the zetabot is equipped with a touch screen display. The display is set on <code class="docutils literal notranslate"><span class="pre">DISPLAY://0</span></code></p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># create video sources and outputs</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">videoSource</span><span class="p">(</span><span class="n">opt</span><span class="o">.</span><span class="n">input_CAMERA</span><span class="p">,</span> <span class="n">argv</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">videoOutput</span><span class="p">(</span><span class="s2">&quot;DISPLAY://0&quot;</span><span class="p">,</span> <span class="n">argv</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span>

<span class="c1"># load the object detection network</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">detectNet</span><span class="p">(</span><span class="n">opt</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">,</span> <span class="n">opt</span><span class="o">.</span><span class="n">threshold</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p class="linemarker linemarker-119">For this task we are utilizing our camera. On our previous trials, we had to to an inference on a single image. The program could recieve the one image infer it with the network and output a single result.</p>
<p class="linemarker linemarker-121">But with a camera, we need to repeatedly run the inference so that we may capture the incoming frames from the camera and output a constant stream of results.</p>
<ul>
<li><p class="linemarker linemarker-123">We may achieve this by running a while loop until an envoked output stream window is killed by the user.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># process frames until the user exits</span>
<span class="k">while</span> <span class="n">display</span><span class="o">.</span><span class="n">IsStreaming</span><span class="p">():</span>
</pre></div>
</div>
</li>
<li><p class="linemarker linemarker-130">Within the while loop:</p>
<ul>
<li><p class="linemarker linemarker-132">Capture the current frame from the camera, run the inference, and determine the estimated detect of the object.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Capture each of the frames of camera</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">camera</span><span class="o">.</span><span class="n">Capture</span><span class="p">()</span>

<span class="c1"># detect objects in the image (with overlay)</span>
<span class="n">detections</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">Detect</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">overlay</span><span class="o">=</span><span class="n">opt</span><span class="o">.</span><span class="n">overlay</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p class="linemarker linemarker-142">It shows the number of detected objects and the name of the detected object</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># print the detections</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;detected </span><span class="si">{:d}</span><span class="s2"> objects in image&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">detections</span><span class="p">)))</span>

<span class="k">for</span> <span class="n">detection</span> <span class="ow">in</span> <span class="n">detections</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">detection</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p class="linemarker linemarker-152">Render the result output and update the title bar of the output window.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># render the image</span>
<span class="n">output</span><span class="o">.</span><span class="n">Render</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

<span class="c1"># update the title bar</span>
<span class="n">output</span><span class="o">.</span><span class="n">SetStatus</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:s}</span><span class="s2"> | Network </span><span class="si">{:.0f}</span><span class="s2"> FPS&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">opt</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">GetNetworkFPS</span><span class="p">()))</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="executing-the-custom-program">
<h2>Executing the Custom Program<a class="headerlink" href="#executing-the-custom-program" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p class="linemarker linemarker-166">Open the <code class="docutils literal notranslate"><span class="pre">02_6-2.</span> <span class="pre">object_detection_camera.py</span></code> notebook.</p></li>
</ul>
<a class=""
               data-lightbox="group-4b71e149-6c0d-4c25-97f3-b3e5795b7a64"
               href="../../../_images/object_detection_camera1.png"
               title=""
               data-title=""
               
               ><img src="../../../_images/object_detection_camera1.png"
                     class=""
                     width="100%"
                     height="auto"
                     alt=""/>
                </a><ul>
<li><p class="linemarker linemarker-172">Run the cell code which initializes the input/ output stream of the environment as well as the CAMERA variable, which will be the flag that determines the input vairable for the program to be a camera stream.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">env</span> <span class="n">DISPLAY</span><span class="o">=</span><span class="p">:</span><span class="mi">0</span>
<span class="o">%</span><span class="n">env</span> <span class="n">csi</span><span class="o">=</span><span class="p">:</span><span class="mi">0</span>
<span class="o">%</span><span class="n">env</span> <span class="n">CAMERA</span><span class="o">=</span><span class="n">csi</span><span class="p">:</span><span class="o">//</span><span class="mi">0</span>
</pre></div>
</div>
</li>
<li><p class="linemarker linemarker-180">Check if your python notebook can read the python code you have written:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cat</span> <span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">zeta</span><span class="o">/</span><span class="n">notebook</span><span class="o">/</span><span class="n">lecture</span><span class="o">/</span><span class="s1">&#39;2.AI Training Examples&#39;</span><span class="o">/</span><span class="s1">&#39;02_6-2. object_detection_camera.py&#39;</span>
</pre></div>
</div>
</li>
<li><p class="linemarker linemarker-186">One important thing about the zetabot is that the Raspberry Pi camera is constantly running.</p>
<p class="linemarker linemarker-188">In order to use the camera for our task we must disable it first by running the following command:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>%%capture
!pm2 stop jetson_camera
</pre></div>
</div>
<p class="linemarker linemarker-195">This will allow the camera to be used for our program.</p>
</li>
<li><p class="linemarker linemarker-197">Execute the pose_estimation_camera python code.</p>
<p class="linemarker linemarker-199"><em>Note</em> that we are setting our major functions,</p>
<ul class="simple">
<li><p class="linemarker linemarker-201"><code class="docutils literal notranslate"><span class="pre">--network</span></code>: to set which networks to use in our pose estimation task.</p></li>
<li><p class="linemarker linemarker-202"><code class="docutils literal notranslate"><span class="pre">input_CAMERA</span></code>: to set which input stream will be used for our task. It is being set to CAMERA environment variable which holds <code class="docutils literal notranslate"><span class="pre">csi://0</span></code> as a string.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>%%capture
!python3 /home/zeta/notebook/lecture/&#39;2.AI Training Examples&#39;/&#39;02_6-2. object_detection_camera.py&#39; --network=ssd-mobilenet-v2 $CAMERA
</pre></div>
</div>
</li>
<li><p class="linemarker linemarker-209">Be sure to turn the camera back online by:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>%%capture
!pm2 start jetson_camera
</pre></div>
</div>
</li>
</ul>
</section>
<section id="let-s-change-the-overlay">
<h2>Let’s Change the Overlay!!!<a class="headerlink" href="#let-s-change-the-overlay" title="Permalink to this heading"></a></h2>
<ul>
<li><p class="linemarker linemarker-220">With the minor functions explanation, we have discussed many parameters that controls the overlay settings. Try to tweak the execution cell in your jupyter notebook to change how the results are overlayed.</p>
<p class="linemarker linemarker-222">Examples:</p>
<a class=""
               data-lightbox="group-55e8877e-7a20-46ac-9155-5a36888da552"
               href="../../../_images/object_detection_camera11.jpg"
               title=""
               data-title=""
               
               ><img src="../../../_images/object_detection_camera11.jpg"
                     class=""
                     width="100%"
                     height="auto"
                     alt=""/>
                </a><div class="line-block">
<div class="line"><br /></div>
</div>
<a class=""
               data-lightbox="group-325a0180-5078-4f30-a7fd-18c48c420954"
               href="../../../_images/object_detection_camera21.jpg"
               title=""
               data-title=""
               
               ><img src="../../../_images/object_detection_camera21.jpg"
                     class=""
                     width="100%"
                     height="auto"
                     alt=""/>
                </a></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="4.coding_explanation.html" class="btn btn-neutral float-left" title="Coding Explanation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="6.discussion.html" class="btn btn-neutral float-right" title="Discussion" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-17821189-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-17821189-2', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>